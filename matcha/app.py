#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import gradio as gr
import torch

from matcha.cli import get_torch_device, load_matcha, load_vocoder, process_text, to_waveform
from matcha.utils.utils import plot_tensor


TITLE="üçµ Matcha-TTS: A fast TTS architecture with conditional flow matching"
DESCRIPTION = """# chuy·ªÉn ƒë·ªïi chuy·ªÉn vƒÉn b·∫£n th√†nh gi·ªçng n√≥i, s·ª≠ d·ª•ng gi·ªçng ƒë·ªçc c·ªßa nh√† vƒÉn Nguy·ªÖn Ng·ªçc Ng·∫°n

original: üçµ https://github.com/shivammehta25/Matcha-TTS

my personal checkpoints: a speech-synthesis model of Vietnamese M.C. Nguy·ªÖn Ng·ªçc Ng·∫°n

the checkpoints is released under CC-BY-NC-SA-4.0

In accordance with the terms of the CC-BY-NC-SA-4.0 license, the use of my checkpoints and any audio output generated by them for commercial purposes is strictly prohibited. This includes, but is not limited to:
- online and offline voice cloning as a service
- online and offline text-to-speech as a service
- content creation for monetization on social media platforms

CƒÉn c·ª© v√†o c√°c ƒëi·ªÅu kho·∫£n c·ªßa gi·∫•p ph√©p CC-BY-NC-SA-4.0, vi·ªác s·ª≠ d·ª•ng c√°c checkpoints n√†y v√† b·∫•t k·ª≥ ƒë·∫ßu ra √¢m thanh n√†o ƒë∆∞·ª£c t·∫°o b·ªüi ch√∫ng ƒë·ªÅu b·ªã nghi√™m c·∫•m s·ª≠ d·ª•ng cho m·ª•c ƒë√≠ch th∆∞∆°ng m·∫°i. ƒêi·ªÅu n√†y bao g·ªìm, nh∆∞ng kh√¥ng gi·ªõi h·∫°n ·ªü:
- c√°c d·ªãch v·ª• nh√¢n b·∫£n gi·ªçng n√≥i tr·ª±c tuy·∫øn v√† ngo·∫°i tuy·∫øn
- c√°c d·ªãch v·ª• chuy·ªÉn vƒÉn b·∫£n th√†nh gi·ªçng n√≥i tr·ª±c tuy·∫øn v√† ngo·∫°i tuy·∫øn
- t·∫°o n·ªôi dung ƒë·ªÉ ki·∫øm ti·ªÅn tr√™n c√°c n·ªÅn t·∫£ng m·∫°ng x√£ h·ªôi"""


DEMO_TEXT_0 = "ƒê√¢y l√† m·ªôt ch∆∞∆°ng tr√¨nh chuy·ªÉn ƒë·ªïi vƒÉn b·∫£n th√†nh gi·ªçng n√≥i, s·ª≠ d·ª•ng gi·ªçng ƒë·ªçc c·ªßa nh√† vƒÉn Nguy·ªÖn Ng·ªçc Ng·∫°n"
DEMO_TEXT_1 = "K√≠nh th∆∞a qu√Ω v·ªã, xin qu√Ω v·ªã gh√© v√†o thƒÉm k√™nh Youtube Nguy·ªÖn Ng·ªçc Ng·∫°n, qu√Ω v·ªã s·∫Ω g·∫∑p l·∫°i t·∫•t c·∫£ c√°c bƒÉng ƒë·ªçc truy·ªán c·ªßa Nguy·ªÖn Ng·ªçc Ng·∫°n do Trung t√¢m Th√∫y Nga th·ª±c hi·ªán v√† nh·ªØng truy·ªán m·ªõi c√πng nh·ªØng bu·ªïi n√≥i chuy·ªán v·ªÅ nhi·ªÅu ƒë·ªÅ t√†i ph·ªï bi·∫øn kh√°c nhau. Xin ch√¢n th√†nh c·∫£m ∆°n v√† ch·ªù ƒë√≥n qu√Ω v·ªã."


DEVICE = get_torch_device()
MODEL = load_matcha("model.ckpt", DEVICE)
VOCODER, DENOISER = load_vocoder("g_02500000", DEVICE)


@torch.inference_mode()
def process_text_gradio(text):
    output = process_text(text, DEVICE)
    return output["x_phones"][1::2], output["x"], output["x_lengths"]


@torch.inference_mode()
def synthesise_mel(text, text_length, n_timesteps, temperature, length_scale):
    output = MODEL.synthesise(text, text_length, n_timesteps=n_timesteps,temperature=temperature, spks=None, length_scale=length_scale)
    waveform = to_waveform(output["mel"], VOCODER, DENOISER).numpy()
    return (22050, waveform), plot_tensor(output["mel"].squeeze().cpu().numpy())
    # sample rate 22.05 kHz


def example_cacher(text, n_timesteps, mel_temp, length_scale):
    phones, text, text_lengths = process_text_gradio(text)
    audio, mel_spectrogram = synthesise_mel(text, text_lengths, n_timesteps, mel_temp, length_scale)
    return phones, audio, mel_spectrogram


with gr.Blocks(title=TITLE, theme="soft") as demo:
    processed_text = gr.State(value=None)
    processed_text_len = gr.State(value=None)

    with gr.Row():
        gr.Markdown(DESCRIPTION)

    with gr.Group():
        with gr.Row():
            text = gr.Textbox(label="nh·∫≠p vƒÉn b·∫£n ti·∫øng vi·ªát", lines=3)
            synth_btn = gr.Button("Chuy·ªÉn th√†nh gi·ªçng n√≥i", variant="primary", scale=0, size="lg")
        with gr.Accordion(label="tu·ª≥ ch·ªçn n√¢ng cao", open=False):
            with gr.Row():
                n_timesteps = gr.Slider(label="s·ªë b∆∞·ªõc xo√° nhi·ªÖu", minimum=1, maximum=100, step=1, value=50, interactive=True)
                length_scale = gr.Slider(label="length scale", info="c√†ng l·ªõn th√¨ ƒë·ªçc c√†ng ch·∫≠m", minimum=0.5, maximum=1.5,  step=0.05, value=.95, interactive=True)
                mel_temp = gr.Slider(label="sampling temperature", minimum=0.00, maximum=2.001, step=0.16675, value=0.667, interactive=True,)
                # idk min max of denoiser strength so let it default at 2.5e-4

    with gr.Group():
        with gr.Row():
            audio = gr.Audio(label="audio", interactive=False)
        with gr.Accordion(label="th√¥ng tin chuy√™n s√¢u", open=False):
            with gr.Row():
                phonetised_text = gr.Textbox(label="VƒÉn b·∫£n d∆∞·ªõi d·∫°ng m·∫´u t·ª± bi·ªÉu √¢m qu·ªëc t·∫ø (IPA)", info="kh·∫©u √¢m H√† N·ªôi", interactive=False, lines=3)
                mel_spectrogram = gr.Image(label="mel spectrogram", interactive=False)

    with gr.Row():
        examples = gr.Examples(
            label="v√≠ d·ª• vƒÉn b·∫£n ƒë·∫ßu v√†o",
            examples=[
                [DEMO_TEXT_0,  2, 0.677, 0.95],
                [DEMO_TEXT_0,  4, 0.677, 0.95],
                [DEMO_TEXT_0, 10, 0.677, 0.95],
                [DEMO_TEXT_0, 50, 0.677, 0.95],
                [DEMO_TEXT_1, 50, 0.677, 0.95],
            ],
            fn=example_cacher,
            inputs=[text, n_timesteps, mel_temp, length_scale],
            outputs=[phonetised_text, audio, mel_spectrogram],
            cache_examples=True,
        )

    with gr.Row():
        gr.Markdown("[![Open In Colab](https://colab.research.google.com/github/phineas-pta/MatchaTTS_ngngngan/blob/main/synthesis.ipynb)")

    synth_btn.click(
        fn=process_text_gradio,
        inputs=[text],
        outputs=[phonetised_text, processed_text, processed_text_len],
        api_name="phonemize",
        queue=True,
    ).then(
        fn=synthesise_mel,
        inputs=[processed_text, processed_text_len, n_timesteps, mel_temp, length_scale],
        outputs=[audio, mel_spectrogram],
        api_name="speak",
    )

demo.queue().launch()
